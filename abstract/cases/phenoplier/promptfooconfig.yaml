prompts:
  - file://../../prompts/baseline.txt
  - file://../../prompts/candidate_with_metadata.txt
  - file://../../prompts/candidate.txt

# Default/standard asserts applied to each test case below
defaultTest:
  assert:
    - type: python
      value: |
        description = "Starts with capital letter"
        type = "Formatting"
        score = 0.25
        
        res = output.strip()[0].isupper()
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

    - type: python
      value: |
        description = "Ends with a period"
        type = "Formatting"
        score = 0.25
        
        res = output.strip()[-1] == "."
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

    - type: python
      value: |
        description = "Is a single paragraph"
        type = "Formatting"
        score = 0.25
        
        res = len(output.strip().split("\n")) == 1
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

    - type: python
      value: |
        description = "Doesn't include manuscript title"
        type = "Formatting"
        score = 0.25
        
        res = (context["vars"]["title"] not in output)
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

    - type: python
      value: |
        description = "Doesn't reference authors, keywords, abstract, etc"
        type = "Formatting"
        score = 0.25
        
        keywords = [
          "authors",
          "Abstract:",
          "Keywords:",
          "References:",
        ]
        res = not any(kw in output for kw in keywords)
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

    - type: python
      value: |
        description = "Has roughly the same length as input"
        type = "Formatting"
        score = 0.25
        
        input = context["vars"]["content"]
        input_words = len(input.strip().split())
        output_words = len(output.strip().split())
        
        res = (output_words > 0.5 * input_words) and (output_words < 1.5 * input_words)
        
        return {
          "pass": res,
          "score": score if res else 0.0,
          "reason": "Passed" if res else "Failed",
          "assertion": {
            "value": description,
            "type": type,
          },
        }

# Specific tests using input texts from the 'inputs' directory
tests:
  - vars:
      test_description: Test case has spelling/grammar errors
      title: file://./inputs/title.txt
      keywords: file://./inputs/keywords.txt
      content: file://./inputs/has_spelling_and_grammar_errors.md
    assert:
      - type: python
        value: |
          description = "Has no spelling/grammar errors"
          type = "Spelling/grammar"
          score = 2.0
          
          # trailing spaces are intentional
          keywords = [
            "concierto",
            "specifico",
            "significantl ",
          ]
          res = not any(kw in output for kw in keywords)
          
          return {
            "pass": res,
            "score": score if res else 0.0,
            "reason": "Passed" if res else "Failed",
            "assertion": {
              "value": description,
              "type": type,
            },
          }

  - vars:
      test_description: Test case has wrong structure, with context at the end
      title: file://./inputs/title.txt
      keywords: file://./inputs/keywords.txt
      content: file://./inputs/content_conclusion_context.md
    assert:
      - type: python
        value: |
          description = "Starts with context"
          type = "Structure"
          score = 2.0
          
          # make sure it does not mention the method ("PhenoPLIER") in the first
          sentence
          res = ("PhenoPLIER" not in output.split(". ")[0])
          
          return {
            "pass": res,
            "score": score if res else 0.0,
            "reason": "Passed" if res else "Failed",
            "assertion": {
              "value": description,
              "type": type,
            },
          }

  # ...describes method before results
  - vars:
      test_description: Provides methods before results
      title: file://./inputs/title.txt
      keywords: file://./inputs/keywords.txt
      content: file://./inputs/doesnt_describe_method_before_results.md
    assert:
      - type: python
        value: |
          description = "Describes method before results"
          type = "Structure"
          score = 2.0
          
          methods_start = output.find("PhenoPLIER")
          results_start = min(
            l if (l := output.find("CRISPR")) > 0 else len(output),
            l if (l := output.find("lipid")) > 0 else len(output),
            l if (l := output.find("drug-disease")) > 0 else len(output),
          )
          res = (methods_start < results_start)
          
          return {
            "pass": res,
            "score": score if res else 0.0,
            "reason": "Passed" if res else "Failed",
            "assertion": {
              "value": description,
              "type": type,
            },
          }

  # ...ends with interpretation of results
  - vars:
      test_description: Ends with interpretation of results
      title: file://./inputs/title.txt
      keywords: file://./inputs/keywords.txt
      content: file://./inputs/doesnt_end_with_interpretation_of_results.md
    assert:
      - type: python
        value: |
          description = "Ends with conclusion"
          type = "Structure"
          score = 2.0
          
          keywords = [
            "overall",
            "conclusion",
            "insight",
            "novel",
            "potential",
            "broader",
            "ultimately",
            "advancement",
            "valuable",
            "our study",
            "this study",
            "our work",
            "this work",
            "our approach",
            "research",
            "critical gap",
            "understanding",
            "powerful",
            "advancing",
            "importance",
            "findings",
            "highlight",
            "promise",
            "promising",
            "innovative",
            "perspective",
            "refine",
            "refining",
            "accelerate",
            "accelerating",
            "facilitate",
            "facilitating",
            "pinpoint",
            "implications",
            "reveal",
          ]
          
          # try to capture the first paragraph (because sometimes models add
          # explanations below the revised paragraph)
          first_paragraph = output.strip().split("\n")[0]
          last_sentences = ".".join(first_paragraph.strip().split(". ")[-1:]).lower()
          res = any(keyword in last_sentences for keyword in keywords)
          
          return {
            "pass": res,
            "score": score if res else 0.0,
            "reason": "Passed" if res else "Failed",
            "assertion": {
              "value": description,
              "type": type,
            },
          }